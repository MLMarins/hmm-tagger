{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Hidden Markov Models\n",
    "---\n",
    "### Introduction\n",
    "\n",
    "In this notebook, you'll use the [Pomegranate](http://pomegranate.readthedocs.io/en/latest/index.html) library to build a simple Hidden Markov Model and explore the Pomegranate API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python modules -- this cell needs to be run again if you make changes to any of the files\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from pomegranate import State, HiddenMarkovModel, DiscreteDistribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Simple HMM\n",
    "---\n",
    "You will start by building a simple HMM network based on an example from the textbook [Artificial Intelligence: A Modern Approach](http://aima.cs.berkeley.edu/).\n",
    "\n",
    "> You are the security guard stationed at a secret under-ground installation. Each day, you try to guess whether itâ€™s raining today, but your only access to the outside world occurs each morning when you see the director coming in with, or without, an umbrella.\n",
    "\n",
    "A simplified diagram of the required network topology is shown below.\n",
    "\n",
    "![](_example.png)\n",
    "\n",
    "### Describing the Network\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "$\\lambda = (A, B)$ specifies a Hidden Markov Model in terms of an emission probability distribution $A$ and a state transition probability distribution $B$.\n",
    "</div>\n",
    "\n",
    "HMM networks are parameterized by two distributions: the emission probabilties giving the conditional probability of observing evidence values for each hidden state, and the transition probabilities giving the conditional probability of moving between states during the sequence. Additionally, you can specify an initial distribution describing the probability of a sequence starting in each state.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "At each time $t$, $X_t$ represents the hidden state, and $Y_t$ represents an observation at that time.\n",
    "</div>\n",
    "\n",
    "In this problem, $t$ corresponds to each day of the week and the hidden state represent the weather outside (whether it is Rainy or Sunny) and observations record whether the security guard sees the director carrying an umbrella or not.\n",
    "\n",
    "For example, during some particular week the guard may observe an umbrella ['yes', 'no', 'yes', 'no', 'yes'] on Monday-Friday, while the weather outside is ['Rainy', 'Sunny', 'Sunny', 'Sunny', 'Rainy']. In that case, $t=Wednesday$, $Y_{Wednesday}=yes$, and $X_{Wednesday}=Sunny$. (It might be surprising that the guard would observe an umbrella on a sunny day, but it is possible under this type of model.)\n",
    "\n",
    "### Initializing an HMM Network with Pomegranate\n",
    "The Pomegranate library supports [two initialization methods](http://pomegranate.readthedocs.io/en/latest/HiddenMarkovModel.html#initialization). You can either explicitly provide the three distributions, or you can build the network line-by-line. We'll use the line-by-line method for the example network, but you're free to use either method for the part of speech tagger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the HMM model\n",
    "model = HiddenMarkovModel(name=\"Example Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IMPLEMENTATION**: Add the Hidden States\n",
    "When the HMM model is specified line-by-line, the object starts as an empty container. The first step is to name each state and attach an emission distribution.\n",
    "\n",
    "#### Observation Emission Probabilities: $P(Y_t | X_t)$\n",
    "We need to assume that we have some prior knowledge (possibly from a data set) about the director's behavior to estimate the emission probabilities for each hidden state. In real problems you can often estimate the emission probabilities empirically, which is what we'll do for the part of speech tagger. Our imaginary data will produce the conditional probability table below. (Note that the rows sum to 1.0)\n",
    "\n",
    "| |  $yes$  | $no$ |\n",
    "| --- | --- | --- |\n",
    "| $Sunny$ |   0.10  | 0.90 |\n",
    "| $Rainy$ | 0.80 | 0.20 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks good so far!\n"
     ]
    }
   ],
   "source": [
    "# create the HMM model\n",
    "model = HiddenMarkovModel(name=\"Example Model\")\n",
    "\n",
    "# emission probability distributions, P(umbrella | weather)\n",
    "sunny_emissions = DiscreteDistribution({\"yes\": 0.1, \"no\": 0.9})\n",
    "sunny_state = State(sunny_emissions, name=\"Sunny\")\n",
    "\n",
    "# TODO: create a discrete distribution for the rainy emissions from the probability table\n",
    "# above & use that distribution to create a state named Rainy\n",
    "rainy_emissions = DiscreteDistribution({\"yes\": 0.8, \"no\": 0.2})\n",
    "rainy_state = State(rainy_emissions, name=\"Rainy\")\n",
    "\n",
    "# add the states to the model\n",
    "model.add_states(sunny_state, rainy_state)\n",
    "\n",
    "assert rainy_emissions.probability(\"yes\") == 0.8, \"The director brings his umbrella with probability 0.8 on rainy days\"\n",
    "print(\"Looks good so far!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **IMPLEMENTATION:** Adding Transitions\n",
    "Once the states are added to the model, we can build up the desired topology of individual state transitions.\n",
    "\n",
    "#### Initial Probability $P(X_0)$:\n",
    "We will assume that we don't know anything useful about the likelihood of a sequence starting in either state. If the sequences start each week on Monday and end each week on Friday (so each week is a new sequence), then this assumption means that it's equally likely that the weather on a Monday may be Rainy or Sunny. We can assign equal probability to each starting state by setting $P(X_0=Rainy) = 0.5$ and $P(X_0=Sunny)=0.5$:\n",
    "\n",
    "| $Sunny$ | $Rainy$ |\n",
    "| --- | ---\n",
    "| 0.5 | 0.5 |\n",
    "\n",
    "#### State transition probabilities $P(X_{t} | X_{t-1})$\n",
    "Finally, we will assume for this example that we can estimate transition probabilities from something like historical weather data for the area. In real problems you can often use the structure of the problem (like a language grammar) to impose restrictions on the transition probabilities, then re-estimate the parameters with the same training data used to estimate the emission probabilities. Under this assumption, we get the conditional probability table below. (Note that the rows sum to 1.0)\n",
    "\n",
    "| | $Sunny$ | $Rainy$ |\n",
    "| --- | --- | --- |\n",
    "|$Sunny$| 0.80 | 0.20 |\n",
    "|$Rainy$| 0.40 | 0.60 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great! You've finished the model.\n"
     ]
    }
   ],
   "source": [
    "# create edges for each possible state transition in the model\n",
    "# equal probability of a sequence starting on either a rainy or sunny day\n",
    "model.add_transition(model.start, sunny_state, 0.5)\n",
    "model.add_transition(model.start, rainy_state, 0.5)\n",
    "\n",
    "# add sunny day transitions (we already know estimates of these probabilities\n",
    "# from the problem statement)\n",
    "model.add_transition(sunny_state, sunny_state, 0.8)  # 80% sunny->sunny\n",
    "model.add_transition(sunny_state, rainy_state, 0.2)  # 20% sunny->rainy\n",
    "\n",
    "# TODO: add rainy day transitions using the probabilities specified in the transition table\n",
    "model.add_transition(rainy_state, sunny_state, 0.4)  # 40% rainy->sunny\n",
    "model.add_transition(rainy_state, rainy_state, 0.6)  # 60% rainy->rainy\n",
    "\n",
    "# finally, call the .bake() method to finalize the model\n",
    "model.bake()\n",
    "\n",
    "assert model.edge_count() == 6, \"There should be two edges from model.start, two from Rainy, and two from Sunny\"\n",
    "assert model.node_count() == 4, \"The states should include model.start, model.end, Rainy, and Sunny\"\n",
    "print(\"Great! You've finished the model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the Model\n",
    "The states of the model can be accessed using array syntax on the `HMM.states` attribute, and the transition matrix can be accessed by calling `HMM.dense_transition_matrix()`. Element $(i, j)$ encodes the probability of transitioning from state $i$ to state $j$. For example, with the default column order specified, element $(2, 1)$ gives the probability of transitioning from \"Rainy\" to \"Sunny\", which we specified as 0.4.\n",
    "\n",
    "Run the next cell to inspect the full state transition matrix, then read the . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The state transition matrix, P(Xt|Xt-1):\n",
      "\n",
      "[[0.  0.5 0.5 0. ]\n",
      " [0.  0.8 0.2 0. ]\n",
      " [0.  0.4 0.6 0. ]\n",
      " [0.  0.  0.  0. ]]\n",
      "\n",
      "The transition probability from Rainy to Sunny is 40%\n"
     ]
    }
   ],
   "source": [
    "column_order = [\"Example Model-start\", \"Sunny\", \"Rainy\", \"Example Model-end\"]  # Override the Pomegranate default order\n",
    "column_names = [s.name for s in model.states]\n",
    "order_index = [column_names.index(c) for c in column_order]\n",
    "\n",
    "# re-order the rows/columns to match the specified column order\n",
    "transitions = model.dense_transition_matrix()[:, order_index][order_index, :]\n",
    "print(\"The state transition matrix, P(Xt|Xt-1):\\n\")\n",
    "print(transitions)\n",
    "print(\"\\nThe transition probability from Rainy to Sunny is {:.0f}%\".format(100 * transitions[2, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference in Hidden Markov Models\n",
    "---\n",
    "Before moving on, we'll use this simple network to quickly go over the Pomegranate API to perform the three most common HMM tasks:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "**Likelihood Evaluation**<br>\n",
    "Given a model $\\lambda=(A,B)$ and a set of observations $Y$, determine $P(Y|\\lambda)$, the likelihood of observing that sequence from the model\n",
    "</div>\n",
    "\n",
    "We can use the weather prediction model to evaluate the likelihood of the sequence [yes, yes, yes, yes, yes] (or any other state sequence). The likelihood is often used in problems like machine translation to weight interpretations in conjunction with a statistical language model.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "**Hidden State Decoding**<br>\n",
    "Given a model $\\lambda=(A,B)$ and a set of observations $Y$, determine $Q$, the most likely sequence of hidden states in the model to produce the observations\n",
    "</div>\n",
    "\n",
    "We can use the weather prediction model to determine the most likely sequence of Rainy/Sunny states for a known observation sequence, like [yes, no] -> [Rainy, Sunny]. We will use decoding in the part of speech tagger to determine the tag for each word of a sentence. The decoding can be further split into \"smoothing\" when we want to calculate past states, \"filtering\" when we want to calculate the current state, or \"prediction\" if we want to calculate future states. \n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "**Parameter Learning**<br>\n",
    "Given a model topography (set of states and connections) and a set of observations $Y$, learn the transition probabilities $A$ and emission probabilities $B$ of the model, $\\lambda=(A,B)$\n",
    "</div>\n",
    "\n",
    "We don't need to learn the model parameters for the weather problem or POS tagging, but it is supported by Pomegranate.\n",
    "\n",
    "### IMPLEMENTATION: Calculate Sequence Likelihood\n",
    "\n",
    "Calculating the likelihood of an observation sequence from an HMM network is performed with the [forward algorithm](https://en.wikipedia.org/wiki/Forward_algorithm). Pomegranate provides the the `HMM.forward()` method to calculate the full matrix showing the likelihood of aligning each observation to each state in the HMM, and the `HMM.log_probability()` method to calculate the cumulative likelihood over all possible hidden state paths that the specified model generated the observation sequence.\n",
    "\n",
    "Fill in the code in the next section with a sample observation sequence and then use the `forward()` and `log_probability()` methods to evaluate the sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Rainy      Sunny      Example Model-start      Example Model-end   \n",
      " <start>      0%         0%               100%                     0%          \n",
      "   yes       40%         5%                0%                      0%          \n",
      "    no        5%        18%                0%                      0%          \n",
      "   yes        5%         2%                0%                      0%          \n",
      "\n",
      "The likelihood over all possible paths of this model producing the sequence ['yes', 'no', 'yes'] is 6.92%\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: input a sequence of 'yes'/'no' values in the list below for testing\n",
    "observations = ['yes', 'no', 'yes']\n",
    "\n",
    "assert len(observations) > 0, \"You need to choose a sequence of 'yes'/'no' observations to test\"\n",
    "\n",
    "# TODO: use model.forward() to calculate the forward matrix of the observed sequence,\n",
    "# and then use np.exp() to convert from log-likelihood to likelihood\n",
    "forward_matrix = np.exp(model.forward(observations))\n",
    "\n",
    "# TODO: use model.log_probability() to calculate the all-paths likelihood of the\n",
    "# observed sequence and then use np.exp() to convert log-likelihood to likelihood\n",
    "probability_percentage = np.exp(model.log_probability(observations))\n",
    "\n",
    "# Display the forward probabilities\n",
    "print(\"         \" + \"\".join(s.name.center(len(s.name)+6) for s in model.states))\n",
    "for i in range(len(observations) + 1):\n",
    "    print(\" <start> \" if i==0 else observations[i - 1].center(9), end=\"\")\n",
    "    print(\"\".join(\"{:.0f}%\".format(100 * forward_matrix[i, j]).center(len(s.name) + 6)\n",
    "                  for j, s in enumerate(model.states)))\n",
    "\n",
    "print(\"\\nThe likelihood over all possible paths \" + \\\n",
    "      \"of this model producing the sequence {} is {:.2f}%\\n\\n\"\n",
    "      .format(observations, 100 * probability_percentage))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPLEMENTATION: Decoding the Most Likely Hidden State Sequence\n",
    "\n",
    "The [Viterbi algorithm](https://en.wikipedia.org/wiki/Viterbi_algorithm) calculates the single path with the highest likelihood to produce a specific observation sequence. Pomegranate provides the `HMM.viterbi()` method to calculate both the hidden state sequence and the corresponding likelihood of the viterbi path.\n",
    "\n",
    "This is called \"decoding\" because we use the observation sequence to decode the corresponding hidden state sequence. In the part of speech tagging problem, the hidden states map to parts of speech and the observations map to sentences. Given a sentence, Viterbi decoding finds the most likely sequence of part of speech tags corresponding to the sentence.\n",
    "\n",
    "Fill in the code in the next section with the same sample observation sequence you used above, and then use the `model.viterbi()` method to calculate the likelihood and most likely state sequence. Compare the Viterbi likelihood against the forward algorithm likelihood for the observation sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most likely weather sequence to have generated these observations is ['Rainy', 'Sunny', 'Rainy'] at 2.30%.\n"
     ]
    }
   ],
   "source": [
    "# TODO: input a sequence of 'yes'/'no' values in the list below for testing\n",
    "observations = ['yes', 'no', 'yes']\n",
    "\n",
    "# TODO: use model.viterbi to find the sequence likelihood & the most likely path\n",
    "viterbi_likelihood, viterbi_path = model.viterbi(observations)\n",
    "\n",
    "print(\"The most likely weather sequence to have generated \" + \\\n",
    "      \"these observations is {} at {:.2f}%.\"\n",
    "      .format([s[1].name for s in viterbi_path[1:]], np.exp(viterbi_likelihood)*100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward likelihood vs Viterbi likelihood\n",
    "Run the cells below to see the likelihood of each sequence of observations with length 3, and compare with the viterbi path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The likelihood of observing ['no', 'no', 'yes'] if the weather sequence is...\n",
      "\t('Sunny', 'Sunny', 'Sunny') is 2.59% \n",
      "\t('Sunny', 'Sunny', 'Rainy') is 5.18%  <-- Viterbi path\n",
      "\t('Sunny', 'Rainy', 'Sunny') is 0.07% \n",
      "\t('Sunny', 'Rainy', 'Rainy') is 0.86% \n",
      "\t('Rainy', 'Sunny', 'Sunny') is 0.29% \n",
      "\t('Rainy', 'Sunny', 'Rainy') is 0.58% \n",
      "\t('Rainy', 'Rainy', 'Sunny') is 0.05% \n",
      "\t('Rainy', 'Rainy', 'Rainy') is 0.58% \n",
      "\n",
      "The total likelihood of observing ['no', 'no', 'yes'] over all possible paths is 10.20%\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "observations = ['no', 'no', 'yes']\n",
    "\n",
    "p = {'Sunny': {'Sunny': np.log(.8), 'Rainy': np.log(.2)}, 'Rainy': {'Sunny': np.log(.4), 'Rainy': np.log(.6)}}\n",
    "e = {'Sunny': {'yes': np.log(.1), 'no': np.log(.9)}, 'Rainy':{'yes':np.log(.8), 'no':np.log(.2)}}\n",
    "o = observations\n",
    "k = []\n",
    "vprob = np.exp(model.viterbi(o)[0])\n",
    "print(\"The likelihood of observing {} if the weather sequence is...\".format(o))\n",
    "for s in product(*[['Sunny', 'Rainy']]*3):\n",
    "    k.append(np.exp(np.log(.5)+e[s[0]][o[0]] + p[s[0]][s[1]] + e[s[1]][o[1]] + p[s[1]][s[2]] + e[s[2]][o[2]]))\n",
    "    print(\"\\t{} is {:.2f}% {}\".format(s, 100 * k[-1], \" <-- Viterbi path\" if k[-1] == vprob else \"\"))\n",
    "print(\"\\nThe total likelihood of observing {} over all possible paths is {:.2f}%\".format(o, 100*sum(k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Congratulations!\n",
    "You've now finished the HMM warmup. You should have all the tools you need to complete the part of speech tagger project."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
